{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quZ0naYCwpr_"
   },
   "source": [
    "<h2> Introduction </h2>\n",
    "This Jupyter Notebook last updated by Shrivats Agrawal (shriv9@seas.upenn.edu) builds on work done by Ssuying Chen (cssuying@umich.edu) and Emily Oxford (eoxford@umich.edu) to train a BERT model.\n",
    "\n",
    "<h5> GDELT: </h5>\n",
    "The GDELT Project (Global Database of Events, Language, and Tone) is the largest, most comprehensive, and highest resolution open database of human society as of now. It connects the world's people, locations, organizations, themes, counts, images, and emotions into a single holistic network over the globe. There are various analysis tools under the GDELT Analysis Service like Event Geographic Network, GKG Heatmap, GKG Word Cloud, and GKG Thematic Timeline.\n",
    "\n",
    "<h2> Aim: </h2>\n",
    "The goal of this script is to provide an effective solution to tag datasets fetched from the AWS GDelt database with the corresponding sectors. In that pursuit a Bidirectional Encoder Representations from Transformers (BERT) model is used for predictions.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MD8z7LvsuZP"
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade pip\n",
    "!pip install tensorflow\n",
    "!pip install torch\n",
    "!pip install transformers\n",
    "# !conda install -yc conda-forge pyarrow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bpclORXA19oi"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pyarrow\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import BertForSequenceClassification\n",
    "import boto3\n",
    "import io\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from IPython.display import clear_output\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----For Reading -----\n",
    "parquet_folder_location = \"ATHENA_BERT_RESULTS/2022/JAN22\"\n",
    "bucket = 'sector-classification'\n",
    "#---For Saving------\n",
    "bucket_save = 'sector-classification'\n",
    "file_location = \"Bert_Results\"\n",
    "year = \"2022\"\n",
    "month = \"JAN\"\n",
    "op_file_format = \"parquet\"\n",
    "\n",
    "#Already Processed Files\n",
    "processed_files_csv = \"ProcessedFiles.csv\"\n",
    "\n",
    "def re_define_folder_location(new_month, new_year):\n",
    "    global month\n",
    "    global year\n",
    "    global parquet_folder_location\n",
    "    month = new_month\n",
    "    year = new_year\n",
    "    parquet_folder_location = f\"ATHENA_BERT_RESULTS/{year}/{month}{year[2:]}\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to read parquet files and return a dataframe\n",
    "def read_parquet_file(bucket, file_key):\n",
    "    print(f\"Reading {file_key}\")\n",
    "    engine = 'pyarrow'\n",
    "    now1 = datetime.now()\n",
    "    s3_client = boto3.client('s3')\n",
    "    obj = s3_client.get_object(Bucket=bucket, Key=file_key)\n",
    "    file = io.BytesIO(obj['Body'].read() )\n",
    "    now2=datetime.now()\n",
    "    print(\"Time for reading file:\",now2-now1)\n",
    "    now1 = datetime.now()\n",
    "    print(\"Trying to read parquet file..\")\n",
    "    df = pd.read_parquet(file)#, engine=engine, use_nullable_dtypes=True)\n",
    "    print(\"parquet file read:\")\n",
    "    now2 = datetime.now()\n",
    "    print(engine, \":\", now2-now1)\n",
    "    return df\n",
    "\n",
    "#Return a list of all parquet files in the folder\n",
    "def return_all_files(bucket, parquet_folder_location):\n",
    "    s3_client = boto3.Session()\n",
    "    s3 = s3_client.resource('s3')\n",
    "    my_bucket = s3.Bucket(bucket)\n",
    "    files_list = []\n",
    "    for file in my_bucket.objects.filter(Prefix= parquet_folder_location):\n",
    "        file_dict = {\"bucket\":file.bucket_name, \"file_key\":file.key}\n",
    "        files_list.append(file_dict)\n",
    "    return files_list\n",
    "\n",
    "#Labels Dict\n",
    "def get_labels_dict():\n",
    "    bucket = \"sector-classification\"\n",
    "    file_key = \"Data/sasb_full_training.xlsx\"\n",
    "\n",
    "\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    obj = s3_client.get_object(Bucket=bucket, Key=file_key)\n",
    "\n",
    "    sasb_data = pd.read_excel(io.BytesIO(obj['Body'].read()))\n",
    "\n",
    "    # sasb_data = pd.read_csv(\"Data/training_data/sasb_full_training.csv\")\n",
    "    sasb_data.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "    labels = sasb_data.Sector.unique()\n",
    "\n",
    "    label_dict = {}\n",
    "    for index, label in enumerate(sorted(labels)):\n",
    "        label_dict[index] = label\n",
    "    return label_dict\n",
    "label_dict = get_labels_dict()\n",
    "\n",
    "#Function to clean and generate column for prediction\n",
    "def obtain_text(row):\n",
    "    replace_points=['.',\"/\",\"\\n\",\"https\",\"http\",\":\",\"www\",\"  \"]\n",
    "    source=str(row['sourcecommonname'])\n",
    "    doc_identifier=str(row['documentidentifier'])\n",
    "    themes=str(row['themes'])\n",
    "    for replace_symbol in replace_points:\n",
    "        source=source.replace(replace_symbol,\" \")\n",
    "        doc_identifier=doc_identifier.replace(replace_symbol,\" \")\n",
    "\n",
    "    try:\n",
    "        themes_text= \" \".join(re.findall(\".*?theme=(.*?),\",themes))\n",
    "    except Exception as e:\n",
    "        themes_text = \"\"\n",
    "  \n",
    "\n",
    "    pred_text=\" \".join([source,doc_identifier,themes_text])\n",
    "    pred_text=pred_text.replace(\"_\",\" \")\n",
    "\n",
    "    text_list=pred_text.split(\" \")\n",
    "    text_list_unique=list(dict.fromkeys(text_list))\n",
    "    pred_text= \" \".join(text_list_unique[:200])\n",
    "  \n",
    "    return pred_text[:1000]\n",
    "\n",
    "#Assigning text labels to categorical numbers\n",
    "def return_labels(label_number):\n",
    "    global label_dict\n",
    "    try:\n",
    "        return label_dict[label_number]\n",
    "    except Exception as e:\n",
    "    #print(e)\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Pre_Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#model.to(device)\n",
    "print(device)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(label_dict),\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)\n",
    "\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('Bert_Models/second_finetuned_BERT_epoch_5.model', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_predictions(df_batch, batch_size):\n",
    "#     print(batch_size)\n",
    "    max_count = df_batch.shape[0]\n",
    "\n",
    "#     f = IntProgress(min=0, max=max_count) # instantiate the bar\n",
    "#     display(f) # display the bar\n",
    "\n",
    "\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                              do_lower_case=True)\n",
    "    predictions_list=[]  \n",
    "#     print(\"Total batches:\", (max_count//batch_size) + 1)\n",
    "    for i in tqdm(range(0,max_count,batch_size)):\n",
    "#       f.value=i\n",
    "        test_data = list(df_batch[i : i + batch_size]['pred_text'])\n",
    "        predicted = [-1 for x in range(len(test_data))]\n",
    "        # sys.exit()\n",
    "        try:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                encoded_data_test = tokenizer.batch_encode_plus(\n",
    "                    #moderna_test, \n",
    "                    test_data[:batch_size],\n",
    "                    add_special_tokens=True, \n",
    "                    return_attention_mask=True, \n",
    "                    padding=True, \n",
    "                    max_length=384,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                #print(encoded_data_test)\n",
    "            #print(encoded_data_test.shape)\n",
    "            input_ids_moderna = encoded_data_test['input_ids']\n",
    "            #print(input_ids_moderna.shape)\n",
    "            attention_masks_moderna = encoded_data_test['attention_mask']\n",
    "\n",
    "            output_moderna = model(input_ids_moderna.to(device))\n",
    "            _, predicted = torch.max(output_moderna[0], 1)\n",
    "            predicted = predicted.tolist()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "        predictions_list.extend(predicted)\n",
    "\n",
    "        if(i%(50*batch_size)==0):\n",
    "            torch.cuda.empty_cache()\n",
    "#             print(i,\"|\",len(predicted),\"| \",len(predictions_list))\n",
    "        \n",
    "    return predictions_list\n",
    "\n",
    "\n",
    "#Saving df to S3\n",
    "def save_df_to_s3(df, bucket_save, file_location, year, month, counter, file_format):\n",
    "    if file_format == \"csv\":\n",
    "        file_name = f\"{month}{year[-2:]}_{counter}.csv\"\n",
    "        file_key = file_location + \"/\" + year + \"/\" + month + \"/\" + file_name\n",
    "        with io.StringIO() as csv_buffer:\n",
    "            s3_client = boto3.client(\"s3\")\n",
    "            df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "            response = s3_client.put_object(\n",
    "                Bucket=bucket_save, Key=file_key, Body=csv_buffer.getvalue()\n",
    "            )\n",
    "\n",
    "            status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "\n",
    "            if status == 200:\n",
    "                print(f\"\\nCSV | Successful S3 put_object response. Status - {status}\\n\")\n",
    "            else:\n",
    "                print(f\"\\nCSV | Unsuccessful S3 put_object response. Status - {status}\\n\")\n",
    "                \n",
    "    elif file_format == \"parquet\":\n",
    "        file_name = f\"{month}{year[-2:]}_{counter}.parquet\"\n",
    "        file_key = file_location + \"/\" + year + \"/\" + month + \"/\" + file_name\n",
    "        with io.BytesIO() as parquet_buffer:\n",
    "            s3_client = boto3.client(\"s3\")\n",
    "            df.to_parquet(parquet_buffer, index=False)\n",
    "\n",
    "            response = s3_client.put_object(\n",
    "                Bucket=bucket_save, Key=file_key, Body=parquet_buffer.getvalue()\n",
    "            )\n",
    "\n",
    "            status = response.get(\"ResponseMetadata\", {}).get(\"HTTPStatusCode\")\n",
    "\n",
    "            if status == 200:\n",
    "                print(f\"\\nParquet | Successful S3 put_object response. Status - {status}\\n\")\n",
    "            else:\n",
    "                print(f\"\\nParquet | Unsuccessful S3 put_object response. Status - {status}\\n\")\n",
    "    else:\n",
    "        raise Exception(\"Incorrect file format specified:\"+file_format)\n",
    "        \n",
    "        \n",
    "#function to add file to csv of processed files\n",
    "def add_file_to_processed_list(file_dict):\n",
    "    file_list = [file_dict]\n",
    "    \n",
    "    df_completed = pd.DataFrame(file_list)\n",
    "    try:\n",
    "        df_prev_completed = pd.read_csv(processed_files_csv)\n",
    "        print(\"Files processed previously:\",df_prev_completed.shape[0])\n",
    "        df_completed = pd.concat([df_prev_completed, df_completed])[list(file_dict.keys())]\n",
    "        df_completed.drop_duplicates(inplace = True)\n",
    "        df_completed.reset_index(drop=True, inplace = True)\n",
    "       \n",
    "        df_completed.to_csv(processed_files_csv)\n",
    "        print(f\"File added! | {file_list}\\nTotal files processed:\",df_completed.shape[0])\n",
    "        \n",
    "    except:\n",
    "        print(csv_file_name,\"doesn't exist. Creating csv!\")\n",
    "        df_completed.to_csv(processed_files_csv)\n",
    "        \n",
    "\n",
    "#Function to check whether a file has already been processed or not\n",
    "def check_file_processed(file_dict):\n",
    "    \n",
    "    file_name = file_dict['file_key']\n",
    "    try:\n",
    "        df_completed = pd.read_csv(processed_files_csv)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "    if file_name in list(df_completed['file_key']):\n",
    "        print(\"File already processed. Skipping!\")\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list = return_all_files(bucket, parquet_folder_location)\n",
    "# files_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________________________________________________________\n",
      "_______________________________________________________________________________\n",
      "File: 0 | ATHENA_BERT_RESULTS/2022/JAN22/20220320_004518_00027_fd2d8_085f08be-3c1b-493d-a9bb-ee5de38b21ab\n",
      "name 'df_batch' is not defined\n",
      "Reading ATHENA_BERT_RESULTS/2022/JAN22/20220320_004518_00027_fd2d8_085f08be-3c1b-493d-a9bb-ee5de38b21ab\n",
      "Time for reading file: 0:00:01.302703\n",
      "Trying to read parquet file..\n",
      "parquet file read:\n",
      "pyarrow : 0:00:24.328337\n",
      "df Shape: (192278, 16)\n",
      "-> Pre-proccesing file\n",
      "Preprocessing time: 0:06:18.027784\n",
      "-> Ready for prediction!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ce9145d045431b9456350b4d3faa00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4006.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2288: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n"
     ]
    }
   ],
   "source": [
    "batch_size = 48\n",
    "months = [month]\n",
    "for month in months:\n",
    "    clear_output()\n",
    "#     #----For Reading -----\n",
    "#     parquet_folder_location = f\"ATHENA_BERT_RESULTS/2021/{month}21\"\n",
    "#     bucket = 'sector-classification'\n",
    "#     files_list = return_all_files(bucket, parquet_folder_location)\n",
    "    \n",
    "    for j in range(len(files_list)):\n",
    "    #     clear_output()\n",
    "\n",
    "\n",
    "        for i in range(2):\n",
    "            print(\"_______________________________________________________________________________\")\n",
    "        file_key = files_list[j]['file_key']\n",
    "        bucket = files_list[j]['bucket']\n",
    "        print(f\"File: {j} | {file_key}\")\n",
    "        if check_file_processed(files_list[j]):\n",
    "            continue\n",
    "\n",
    "\n",
    "        #Memory clear step\n",
    "        try:\n",
    "            del df_batch\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "        for _ in range(3):\n",
    "            gc.collect()\n",
    "            time.sleep(1)\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "        df_batch = read_parquet_file(bucket, file_key)\n",
    "        #TEMP\n",
    "    #     df_batch = df_batch[0:500]\n",
    "        print(\"df Shape:\",df_batch.shape)\n",
    "        print(\"-> Pre-proccesing file\")\n",
    "        now1 = datetime.now()\n",
    "        df_batch['pred_text']=df_batch.apply(obtain_text,axis=1)\n",
    "        now2 = datetime.now()\n",
    "        print(\"Preprocessing time:\",now2-now1)\n",
    "\n",
    "        print(\"-> Ready for prediction!\")\n",
    "        #Making predictions\n",
    "        predictions_list = make_predictions(df_batch, batch_size)\n",
    "        print(\"-> Prediction Complete. \\n->Preparing to save file.\")\n",
    "        now3 = datetime.now()\n",
    "        df_batch['SASB_Tag'] = predictions_list\n",
    "        df_batch['Predicted_Sector'] = df_batch['SASB_Tag'].apply(return_labels)\n",
    "        df_batch = df_batch[df_batch['SASB_Tag']!=-1].reset_index(drop=True)\n",
    "        save_df_to_s3(df_batch, bucket_save, file_location, year, month, j,op_file_format)\n",
    "        now4 = datetime.now()\n",
    "        print(\"->File saved! Time for saving file:\",now4 - now3)\n",
    "\n",
    "        #Adding file to processed list\n",
    "        add_file_to_processed_list(files_list[j])\n",
    "        #function to save the dataframe at the given location\n",
    "    #     break\n",
    "\n",
    "        time.sleep(5)\n",
    "    #     if j==2:\n",
    "    #         break\n",
    "# df_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Shrivats-BERT_doc_classifier.ipynb",
   "provenance": []
  },
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

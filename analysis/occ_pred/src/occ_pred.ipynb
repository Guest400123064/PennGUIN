{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Any, Dict\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import pipeline\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "MODEL_CARD = 'valhalla/distilbart-mnli-12-1'\n",
    "PIPELINE = pipeline(\n",
    "    'zero-shot-classification', MODEL_CARD,\n",
    "    device=(0 if torch.cuda.is_available() else -1)\n",
    ")\n",
    "\n",
    "\n",
    "def batched_prediction(batch, candidate_labels: List[str], col_text: str = 'text'):\n",
    "    \n",
    "    out = PIPELINE(batch[col_text], candidate_labels=candidate_labels)\n",
    "    ret = {'predictions': out}\n",
    "    return ret\n",
    "\n",
    "\n",
    "def clean_str(s: str) -> str:\n",
    "    \"\"\"String pre-processing function, used to reduce noise.\n",
    "        1. Convert all characters to ASCII\n",
    "        2. Remove other irrelevant stuff like email address or external url\n",
    "        3. Remove special symbols like newline character \\\\n\"\"\"\n",
    "        \n",
    "    # Normalize special chars\n",
    "    s = str(s)\n",
    "    s = (unicodedata.normalize('NFKD', s)\n",
    "            .encode('ascii', 'ignore').decode())\n",
    "\n",
    "    # Remove irrelevant info\n",
    "    s = re.sub(r'\\S*@\\S*\\s?', '', s)     # Email\n",
    "    s = re.sub(r'\\S*https?:\\S*', '', s)  # URL (http)\n",
    "    s = re.sub(r'\\S*www\\.\\S*', '', s)    # URL (www)\n",
    "    \n",
    "    # Keep punctuation and words only\n",
    "    pattern_keep = (string.punctuation + \n",
    "                        string.ascii_letters + \n",
    "                        string.digits + \n",
    "                        r' ')\n",
    "    return re.sub(r'[^' + pattern_keep + r']+', '', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5be450e01bc4554a5d0ca53607d5ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load occupation categories\n",
    "df_occ = pd.read_csv('../data/categories.csv')\n",
    "df_occ.loc[:, 'occupation'] = df_occ.loc[:, 'occupation'].str.lower()\n",
    "\n",
    "# Load textual descriptions of interested entities\n",
    "df_ent = pd.read_csv('../data/Search_Region_NI.csv')\n",
    "df_ent = df_ent.loc[~df_ent.loc[:, 'org_flag']]\n",
    "df_ent.loc[:, 'text'] = df_ent.loc[:, 'description1'].map(clean_str)\n",
    "\n",
    "# Convert from Pandas to Huggingface dataset and predict occupations\n",
    "df_ent = (Dataset\n",
    "    .from_pandas(df_ent)\n",
    "    .map(\n",
    "        batched_prediction, \n",
    "        batched=True,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        fn_kwargs={'candidate_labels': df_occ.occupation.unique().tolist()}\n",
    "    )\n",
    "    .to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_keep = {\n",
    "    'politician',\n",
    "    'businessperson',\n",
    "    'journalist',\n",
    "    'social activist',\n",
    "    'extremist',\n",
    "    'judge',\n",
    "    'lawyer',\n",
    "    'economist',\n",
    "    'critic',\n",
    "    'military personnel'\n",
    "}\n",
    "\n",
    "df_ent.loc[:, 'top1_label'] = df_ent.predictions.map(lambda d: d['labels'][0])\n",
    "df_ent.loc[:, 'top1_score'] = df_ent.predictions.map(lambda d: d['scores'][0])\n",
    "df_ent.loc[:, 'is_kept'] = df_ent.top1_label.map(lambda o: int(o in occ_keep))\n",
    "df_ent.to_csv('../out/Search_Region_NI_pred_occ_bart.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of extremist predictions\n",
    "# df_ext = pd.read_csv('../out/SF_all_tone_2k_entities_pred_occ_bart.csv')\n",
    "# df_ext = df_ext.loc[df_ext.occ_pred == 'extremist', ['entity', 'description1', 'description2']].reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9408d1441d072873d2a4ce5321f6d40e4e7ee5a37a40f5d992ebe56b872968bb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('coref')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
